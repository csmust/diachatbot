{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhou/.conda/envs/clb2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 209\u001b[0m\n\u001b[1;32m    207\u001b[0m     path\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39margv[\u001b[39m1\u001b[39m]\n\u001b[1;32m    208\u001b[0m \u001b[39m# preprocess('User',path)\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m preprocess(\u001b[39m'\u001b[39;49m\u001b[39mAll\u001b[39;49m\u001b[39m'\u001b[39;49m,path)\n\u001b[1;32m    210\u001b[0m \u001b[39m# preprocess('Doctor',path)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(mode, tokenizerpath, CROSS_TRAIN)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(mode,tokenizerpath,CROSS_TRAIN\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m     \u001b[39massert\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mUser\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDoctor\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m---> 24\u001b[0m     cur_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39;49m))\n\u001b[1;32m     25\u001b[0m     data_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cur_dir, \u001b[39m'\u001b[39m\u001b[39m../../../../data/diachat/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m     processed_data_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cur_dir, \u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_data\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(mode))\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# 导入模块\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 解压函数\n",
    "def read_zipped_json(filepath, filename):\n",
    "    archive = zipfile.ZipFile(filepath, 'r')\n",
    "    return json.load(archive.open(filename))\n",
    "\n",
    "\n",
    "# 预处理函数\n",
    "def preprocess(mode,tokenizerpath,CROSS_TRAIN=True):\n",
    "    assert mode == 'All' or mode == 'User' or mode == 'Doctor' \n",
    "    cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    data_dir = os.path.join(cur_dir, '../../../../data/diachat/')\n",
    "    processed_data_dir = os.path.join(cur_dir, 'data/{}_data'.format(mode))\n",
    "\n",
    "    if not os.path.exists(processed_data_dir):\n",
    "        os.makedirs(processed_data_dir)\n",
    "    data_key = ['train', 'val', 'test']\n",
    "    if CROSS_TRAIN :\n",
    "        data_key=['train', 'val']\n",
    "    data = {}\n",
    "    for key in data_key:\n",
    "        data[key] = read_zipped_json(os.path.join(data_dir, key + '.json.zip'), key + '.json')\n",
    "        print('load {}, size {}'.format(key, len(data[key])))\n",
    "\n",
    "    processed_data = {}\n",
    "    all_intent = []\n",
    "    all_tag = []\n",
    "    all_act=[]\n",
    "    all_domain=[]\n",
    "    all_golden=[]\n",
    "    all_dis=[]\n",
    "    all_strdis=[]\n",
    "\n",
    "    context_size = 3\n",
    "\n",
    "    if tokenizerpath:\n",
    "        try:\n",
    "            tokenizer = BertTokenizer.from_pretrained(tokenizerpath)  # tokenizerpath就是bert模型所在路径，这里就是自己训练的bert模型所在路径\n",
    "            print(\"tokenizer的路径为:{}\".format(tokenizerpath))\n",
    "        except:\n",
    "            print(\"请传入正确的tokenizerpath,比如python .\\preprocess.py E:/Local-Data/MedDialogueGenNew/output/mlm/01/model不传入则默认为hfl/chinese-bert-wwm-ext\")\n",
    "    else:\n",
    "#         try:\n",
    "#             tokenizer = BertTokenizer.from_pretrained(\"E:/Local-Data/models_datasets/chinese-bert-wwm-ext\") # remote108\n",
    "#             print(\"remote108 tokenizer E:/Local-Data/models_datasets/chinese-bert-wwm-ext \")\n",
    "#         except:\n",
    "        '''\n",
    "        hfl/chinese-bert-wwm-ext 是 https://huggingface.co/hfl 页面 名为hfl/chinese-bert-wwm-ext的bert模型\n",
    "        BertTokenizer可自行下载并加载\n",
    "                    也可直接在页面https://huggingface.co/hfl/chinese-bert-wwm-ext下载\n",
    "        '''\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    for key in data_key:\n",
    "        processed_data[key] = []\n",
    "        for sess in data[key]:\n",
    "            context = []\n",
    "            for i, turn in enumerate(sess['utterances']):\n",
    "                if mode == 'User' and turn['agentRole'] == 'Doctor':\n",
    "                    context.append(turn['utterance'])\n",
    "                    continue\n",
    "                elif mode == 'Doctor' and turn['agentRole'] == 'User':\n",
    "                    context.append(turn['utterance'])\n",
    "                    continue\n",
    "                utterance = turn['utterance']\n",
    "                # Notice: ## prefix, space remove\n",
    "                tokens = tokenizer.tokenize(utterance)\n",
    "\n",
    "                golden = []\n",
    "                dis=[]\n",
    "\n",
    "                span_info = []\n",
    "                intents = []\n",
    "                domain=[]\n",
    "                act=[]\n",
    "\n",
    "                for i in turn['annotation']:\n",
    "                    for j in i['slot_values']:\n",
    "                        if j['value'] is not None and j['value'] != '' and j['value'] != '？' and j['value'] != '?':\n",
    "                            if j['value'] in utterance:\n",
    "                                idx = utterance.index(j['value'])   # 有点小bug  value【吃药】 在utterance中重复出现时候比如 \"我现在没有吃药，不需要吃药吗？\" 我们需要标注后一个吃药时\n",
    "                                idx = len(tokenizer.tokenize(utterance[:idx]))\n",
    "                                span_info.append((\n",
    "                                    '+'.join([i['act_label'], j['domain'], j['slot']]), idx,\n",
    "                                    idx + len(tokenizer.tokenize(j['value'])),\n",
    "                                    j['value']))\n",
    "                                token_v = ''.join(tokens[idx:idx + len(tokenizer.tokenize(j['value']))])\n",
    "                                # if token_v != j['value']:    #\n",
    "                                #     print(\"--- token_v != j['value'] ---\")    # '5—7' 应该为\"5-7\"  短杠问题 token 被记为 5[UNK]7\n",
    "                                #     print(j['slot'], token_v, j['value'],\"\\n\")\n",
    "                                # if \"##\" in token_v:\n",
    "                                #     print(\"--- ## in token_v ---\")\n",
    "                                #     print(j['slot'], token_v, j['value'],\"\\n\")\n",
    "                                token_v = token_v.replace('##', '')\n",
    "                                golden.append([i['act_label'], j['domain'], j['slot'], token_v])\n",
    "                                domain.append(j['domain'])\n",
    "                                act.append(i['act_label'])\n",
    "                            else:\n",
    "                                # print(\"--- value不在utterance中 ---\")\n",
    "                                # print(j['value'], utterance,\"\\n\")\n",
    "                                golden.append([i['act_label'], j['domain'], j['slot'], j['value']])\n",
    "                                domain.append(j['domain'])\n",
    "                                act.append(i['act_label'])\n",
    "                        else:\n",
    "                            intents.append('+'.join([i['act_label'], j['domain'], j['slot'], j['value']]))\n",
    "                            golden.append([i['act_label'], j['domain'], j['slot'], j['value']])\n",
    "                            domain.append(j['domain'])\n",
    "                            act.append(i['act_label'])\n",
    "                        temp=[i['act_label'], j['domain'], j['slot']]\n",
    "                        if temp not in dis:\n",
    "                            dis.append(temp.copy())\n",
    "\n",
    "                tags = []\n",
    "                for j, _ in enumerate(tokens):\n",
    "                    for span in span_info:\n",
    "                        if j == span[1]:\n",
    "                            tag = \"B+\" + span[0]\n",
    "                            tags.append(tag)\n",
    "                            break\n",
    "                        if span[1] < j < span[2]:\n",
    "                            tag = \"I+\" + span[0]\n",
    "                            tags.append(tag)\n",
    "                            break\n",
    "                    else:\n",
    "                        tags.append(\"O\")\n",
    "\n",
    "                processed_data[key].append([tokens, tags, intents, golden, context[-context_size:]])\n",
    "                # print([tokens, tags, intents, golden, context[-context_size:]])\n",
    "                # input()\n",
    "\n",
    "                all_intent += intents\n",
    "                all_tag += tags\n",
    "                all_domain+=domain\n",
    "                all_act+=act\n",
    "                for i in golden:\n",
    "                    if i not in all_golden:\n",
    "                        all_golden += [i.copy()]\n",
    "                for j in dis:\n",
    "                    '''j=  [\n",
    "                            \"Inform\",\n",
    "                            \"饮食\",\n",
    "                            \"饮食名\"\n",
    "                        ],'''\n",
    "                    strj=j[0]+j[1]+j[2]\n",
    "                    all_strdis.append(strj)\n",
    "                    if j not in all_dis:\n",
    "                        all_dis += [j.copy()]\n",
    "                    \n",
    "\n",
    "                context.append(turn['utterance'])\n",
    "\n",
    "        all_intent = [x[0] for x in dict(Counter(all_intent)).items()]\n",
    "        all_tag = [x[0] for x in dict(Counter(all_tag)).items()]\n",
    "        all_domain=[x[0] for x in dict(Counter(all_domain)).items()]\n",
    "        all_act=[x[0] for x in dict(Counter(all_act)).items()]\n",
    "        # all_golden = [x[0] for x in dict(Counter(all_golden)).items()]\n",
    "        strdis_count=dict(Counter(all_strdis))\n",
    "        \n",
    "\n",
    "\n",
    "        print('loaded {}, size {}'.format(key, len(processed_data[key])))\n",
    "        json.dump(processed_data[key],\n",
    "                  open(os.path.join(processed_data_dir, '{}_data.json'.format(key)), 'w', encoding='utf-8'),\n",
    "                  indent=2, ensure_ascii=False)\n",
    "\n",
    "    print('sentence label num:', len(all_intent))\n",
    "    print('tag num:', len(all_tag))\n",
    "    print(all_intent)\n",
    "    json.dump(all_intent, open(os.path.join(processed_data_dir, 'intent_vocab.json'), 'w', encoding='utf-8'), indent=2,\n",
    "              ensure_ascii=False)\n",
    "    json.dump(all_tag, open(os.path.join(processed_data_dir, 'tag_vocab.json'), 'w', encoding='utf-8'), indent=2,\n",
    "              ensure_ascii=False)\n",
    "    json.dump(all_domain, open(os.path.join(processed_data_dir, 'domain_vocab.json'), 'w', encoding='utf-8'), indent=2,\n",
    "              ensure_ascii=False)\n",
    "    json.dump(all_act, open(os.path.join(processed_data_dir, 'act_vocab.json'), 'w', encoding='utf-8'), indent=2,\n",
    "              ensure_ascii=False)\n",
    "    json.dump(all_golden, open(os.path.join(processed_data_dir, 'golden_vocab.json'), 'w', encoding='utf-8'), indent=2,\n",
    "                ensure_ascii=False)\n",
    "    json.dump(all_dis, open(os.path.join(processed_data_dir, 'dis_vocab.json'), 'w', encoding='utf-8'), indent=2,\n",
    "                ensure_ascii=False)\n",
    "    json.dump(strdis_count, open(os.path.join(processed_data_dir, 'strdis_count.json'), 'w', encoding='utf-8'), indent=2,\n",
    "                ensure_ascii=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "  该参数与config文件的 pretrained_weights参数值保持一致。比如\n",
    "    “python preprocess.py E:/Local-Data/models_datasets/chinese-bert-wwm-ext”, \n",
    "    不加参数默认为原hfl/chinese-bert-wwm-ext\n",
    "   模型训练和加载的时候选用对应的config文件，以便加载对那个的bert模型\n",
    "    '''\n",
    "    path=\"\"\n",
    "    if len(sys.argv)>1:\n",
    "        path=sys.argv[1]\n",
    "    # preprocess('User',path)\n",
    "    preprocess('All',path)\n",
    "    # preprocess('Doctor',path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
